# utils_pdf_vector.py 
import chromadb
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import streamlit as st
import os
from typing import List, Dict, Optional

# pysqlite3 ê´€ë ¨ ì½”ë“œ ì™„ì „ ì œê±°

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
embedder = None
collection = None
chroma_client = None

def initialize_vector_system():
    """ë²¡í„° ì‹œìŠ¤í…œ ì´ˆê¸°í™” - í•„ìš”í•  ë•Œë§Œ í˜¸ì¶œ"""
    global embedder, collection, chroma_client
    
    if embedder is not None and collection is not None:
        return True
    
    try:
        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ê°€ë³ê³  ë¬´ë£Œ)
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        
        # 2. ChromaDB ì¸ìŠ¤í„´ìŠ¤
        chroma_client = chromadb.Client()
        collection = chroma_client.get_or_create_collection("pdf_chunks")
        
        return True
    except Exception as e:
        st.error(f"âŒ ë²¡í„° ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

def search_pdf_chunks(query: str, pdf_id: str = "default", top_k: int = 3) -> str:
    """ê³ ê¸‰ PDF ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜ - í˜¸í™˜ì„± ê°œì„ """
    
    # ë²¡í„° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if not initialize_vector_system():
        st.warning("âš ï¸ ê³ ê¸‰ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê°„ë‹¨ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
        return fallback_to_simple_search(query, pdf_id, top_k)
    
    try:
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        q_embed = embedder.encode([query])[0].tolist()
        
        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        result = collection.query(
            query_embeddings=[q_embed], 
            n_results=top_k
        )
        
        # ê²°ê³¼ ì²˜ë¦¬
        if (result and "documents" in result and 
            result["documents"][0] and len(result["documents"][0]) > 0):
            
            chunks = result["documents"][0]
            formatted_chunks = []
            
            for i, chunk in enumerate(chunks, 1):
                # ì²­í¬ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ)
                if len(chunk) > 500:
                    chunk = chunk[:500] + "..."
                formatted_chunks.append(f"ğŸ“„ ê³ ê¸‰ ê²€ìƒ‰ ê²°ê³¼ {i}:\n{chunk}")
            
            return "\n---\n".join(formatted_chunks)
        else:
            st.info("â„¹ï¸ ê³ ê¸‰ ê²€ìƒ‰ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "[ê³ ê¸‰ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ]"
            
    except Exception as e:
        st.error(f"âŒ ê³ ê¸‰ PDF ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return fallback_to_simple_search(query, pdf_id, top_k)

def fallback_to_simple_search(query: str, pdf_id: str, top_k: int) -> str:
    """ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±"""
    try:
        # ê°„ë‹¨ ê²€ìƒ‰ ë¡œì§
        if 'pdf_chunks' not in st.session_state or pdf_id not in st.session_state.pdf_chunks:
            return "[PDFê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.]"
        
        text = st.session_state.pdf_chunks[pdf_id]
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        import re
        keywords = re.findall(r'\w+', query.lower())
        paragraphs = text.split('\n\n')
        
        scored_paragraphs = []
        for para in paragraphs:
            if len(para.strip()) < 50:
                continue
                
            para_lower = para.lower()
            score = sum(1 for keyword in keywords if keyword in para_lower)
            
            if score > 0:
                scored_paragraphs.append((score, para.strip()))
        
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        results = []
        for i, (score, para) in enumerate(scored_paragraphs[:top_k], 1):
            if len(para) > 500:
                para = para[:500] + "..."
            results.append(f"ğŸ“„ ê°„ë‹¨ ê²€ìƒ‰ ê²°ê³¼ {i} (ê´€ë ¨ë„: {score}):\n{para}")
        
        if results:
            return "\n---\n".join(results)
        else:
            return "[ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.]"
            
    except Exception as e:
        st.error(f"âŒ ê°„ë‹¨ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return "[ê²€ìƒ‰ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.]"

def pdf_to_chunks(pdf_path: str, chunk_size: int = 400) -> List[str]:
    """PDFë¥¼ ì²­í¬ë¡œ ë¶„í• """
    try:
        doc = fitz.open(pdf_path)
        chunks = []
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text = page.get_text()
            
            # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = text.split('. ')
            
            current_chunk = ""
            for sentence in sentences:
                if len(current_chunk) + len(sentence) < chunk_size:
                    current_chunk += sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                chunks.append(current_chunk.strip())
        
        doc.close()
        return chunks
        
    except Exception as e:
        st.error(f"âŒ PDF ì²­í¬ ë¶„í•  ì˜¤ë¥˜: {e}")
        return []

def save_pdf_chunks_to_chroma(pdf_path: str, pdf_id: str = "default") -> bool:
    """PDF ì²­í¬ë¥¼ ChromaDBì— ì €ì¥"""
    try:
        # ë²¡í„° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if not initialize_vector_system():
            st.warning("âš ï¸ ë²¡í„° ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê°„ë‹¨ ì €ì¥ìœ¼ë¡œ ì „í™˜")
            return fallback_to_simple_save(pdf_path, pdf_id)
        
        # PDFë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = pdf_to_chunks(pdf_path)
        
        if not chunks:
            st.error("âŒ PDF ì²­í¬ ë¶„í•  ì‹¤íŒ¨")
            return False
        
        # ì²­í¬ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ê°„ë‹¨ ê²€ìƒ‰ìš©)
        if 'pdf_chunks' not in st.session_state:
            st.session_state.pdf_chunks = {}
        
        st.session_state.pdf_chunks[pdf_id] = "\n\n".join(chunks)
        
        # ChromaDBì— ì €ì¥
        try:
            # ì„ë² ë”© ìƒì„±
            embeddings = embedder.encode(chunks)
            
            # ChromaDBì— ì €ì¥
            collection.add(
                embeddings=embeddings.tolist(),
                documents=chunks,
                ids=[f"{pdf_id}_{i}" for i in range(len(chunks))]
            )
            
            st.success(f"âœ… PDF ì²­í¬ {len(chunks)}ê°œê°€ ChromaDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            st.error(f"âŒ ChromaDB ì €ì¥ ì‹¤íŒ¨: {e}")
            return fallback_to_simple_save(pdf_path, pdf_id)
        
    except Exception as e:
        st.error(f"âŒ PDF ì €ì¥ ì˜¤ë¥˜: {e}")
        return False

def fallback_to_simple_save(pdf_path: str, pdf_id: str) -> bool:
    """ChromaDB ì €ì¥ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨ ì €ì¥ìœ¼ë¡œ í´ë°±"""
    try:
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_pdf(pdf_path)
        
        if not text:
            return False
        
        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        if 'pdf_chunks' not in st.session_state:
            st.session_state.pdf_chunks = {}
        
        st.session_state.pdf_chunks[pdf_id] = text
        st.success(f"âœ… PDFê°€ ê°„ë‹¨ ëª¨ë“œë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
        
    except Exception as e:
        st.error(f"âŒ ê°„ë‹¨ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def extract_text_from_pdf(pdf_path: str) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = fitz.open(pdf_path)
        text = ""
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text += page.get_text() + "\n"
        
        doc.close()
        return text
        
    except Exception as e:
        st.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def get_pdf_summary(pdf_id: str = "default") -> str:
    """PDF ìš”ì•½ ì •ë³´ ë°˜í™˜"""
    if 'pdf_chunks' not in st.session_state or pdf_id not in st.session_state.pdf_chunks:
        return "[PDF ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.]"
    
    text = st.session_state.pdf_chunks[pdf_id]
    return text[:1000] + "..." if len(text) > 1000 else text