# summary_generator.py

import dspy
from dspy import Signature, InputField, OutputField
import re
from datetime import datetime
from typing import Dict, List, Optional, Any
import streamlit as st
import time
import random
import anthropic

# === Rate Limiting ë° ì¬ì‹œë„ ì„¤ì • ===
MAX_RETRIES = 5
BASE_WAIT_TIME = 60  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
MAX_WAIT_TIME = 300  # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

class RateLimitHandler:
    """Rate Limit ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""
    
    @staticmethod
    def handle_rate_limit_error(error, attempt: int) -> bool:
        """Rate limit ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
        if "rate_limit_error" in str(error) or "RateLimitError" in str(error):
            # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„° ì ìš©
            wait_time = min(BASE_WAIT_TIME * (2 ** attempt) + random.uniform(0, 30), MAX_WAIT_TIME)
            
            st.warning(f"âš ï¸ API ì†ë„ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. {wait_time:.0f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {attempt + 1}/{MAX_RETRIES})")
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” í‘œì‹œ
            progress_bar = st.progress(0)
            for i in range(int(wait_time)):
                time.sleep(1)
                progress_bar.progress((i + 1) / int(wait_time))
            progress_bar.empty()
            
            return True  # ì¬ì‹œë„
        return False  # ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
    
    @staticmethod
    def handle_overloaded_error(error, attempt: int) -> bool:
        """ê³¼ë¶€í•˜ ì˜¤ë¥˜ ì²˜ë¦¬"""
        if "overloaded_error" in str(error) or "Overloaded" in str(error):
            wait_time = min(30 * (3 ** attempt) + random.uniform(10, 60), MAX_WAIT_TIME)
            st.warning(f"âš ï¸ API ì„œë²„ ê³¼ë¶€í•˜. {wait_time:.0f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {attempt + 1}/{MAX_RETRIES})")
            
            progress_bar = st.progress(0)
            for i in range(int(wait_time)):
                time.sleep(1)
                progress_bar.progress((i + 1) / int(wait_time))
            progress_bar.empty()
            
            return True
        return False
    


# === DSPy Signature í´ë˜ìŠ¤ë“¤ ===

class SiteAnalysisFields(Signature):
    text: str = InputField(desc="PDFì—ì„œ ì¶”ì¶œí•œ ì „ì²´ í…ìŠ¤íŠ¸")
    site_area: str = OutputField(desc="ëŒ€ì§€ë©´ì ")
    site_address: str = OutputField(desc="ëŒ€ì§€ ì£¼ì†Œ")
    site_slope: str = OutputField(desc="ëŒ€ì§€ ê²½ì‚¬, ê³ ë„, ë°©ìœ„")
    zoning: str = OutputField(desc="ìš©ë„ì§€ì—­, ì§€êµ¬ë‹¨ìœ„ê³„íš ë“±")
    restrictions: str = OutputField(desc="ê³ ë„ì œí•œ, ì¼ì¡°ê¶Œ, í™˜ê²½, ì†ŒìŒ, íŠ¹ì´ ê·œì œ")
    traffic: str = OutputField(desc="ì£¼ë³€ ë„ë¡œ, êµí†µ, ì§„ì¶œì…")
    precedent_comparison: str = OutputField(desc="ìœ ì‚¬ ì—°ìˆ˜ì›Â·êµìœ¡ì‹œì„¤ê³¼ ë¹„êµ í¬ì¸íŠ¸")
    risk_factors: str = OutputField(desc="ëŒ€ì§€Â·ë²•ê·œ ê´€ë ¨ ì£¼ìš” ë¦¬ìŠ¤í¬")

class PDFSummary(Signature):
    text: str = InputField(desc="PDFì—ì„œ ì¶”ì¶œí•œ ì „ì²´ í…ìŠ¤íŠ¸")
    summary: str = OutputField(desc="PDF ë‚´ìš©ì˜ í•µì‹¬ ìš”ì•½")

class QualityCheck(Signature):
    text: str = InputField(desc="ì¶”ì¶œëœ í…ìŠ¤íŠ¸")
    quality_score: str = OutputField(desc="í’ˆì§ˆ ì ìˆ˜ (0-100)")
    confidence_level: str = OutputField(desc="ì‹ ë¢°ë„ (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)")
    missing_info: str = OutputField(desc="ëˆ„ë½ëœ ì •ë³´ ëª©ë¡")

class PDFTypeDetector(Signature):
    text: str = InputField(desc="PDFì—ì„œ ì¶”ì¶œí•œ ì „ì²´ í…ìŠ¤íŠ¸")
    pdf_type: str = OutputField(desc="PDF ìœ í˜• (architectural_plan/land_use_plan/environmental_assessment/general_document)")
    document_category: str = OutputField(desc="ë¬¸ì„œ ì¹´í…Œê³ ë¦¬")

# === ê³ ê¸‰ PDF ë¶„ì„ê¸° í´ë˜ìŠ¤ ===

class AdvancedPDFAnalyzer:
    def __init__(self):
        """ê³ ê¸‰ PDF ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.site_parser = dspy.Predict(SiteAnalysisFields)
        self.summary_predictor = dspy.Predict(PDFSummary)
        self.quality_checker = dspy.Predict(QualityCheck)
        self.type_detector = dspy.Predict(PDFTypeDetector)
        
        # í•„ìˆ˜ í•„ë“œ ì •ì˜
        self.required_fields = [
            "site_area", "site_address", "site_slope", "zoning", 
            "restrictions", "traffic", "precedent_comparison", "risk_factors"
        ]
        
        # ê¸°ë³¸ê°’ ì •ì˜
        self.default_values = {
            "site_area": "ëŒ€ì§€ë©´ì  ì •ë³´ ì—†ìŒ",
            "site_address": "ëŒ€ì§€ ì£¼ì†Œ ì •ë³´ ì—†ìŒ",
            "site_slope": "ëŒ€ì§€ ê²½ì‚¬ ì •ë³´ ì—†ìŒ",
            "zoning": "ìš©ë„ì§€ì—­ ì •ë³´ ì—†ìŒ",
            "restrictions": "ê±´ì¶• ê·œì œ ì •ë³´ ì—†ìŒ",
            "traffic": "êµí†µ ì •ë³´ ì—†ìŒ",
            "precedent_comparison": "ìœ ì‚¬ ì‚¬ë¡€ ë¹„êµ ì •ë³´ ì—†ìŒ",
            "risk_factors": "ë¦¬ìŠ¤í¬ ìš”ì¸ ì •ë³´ ì—†ìŒ"
        }
    
    def detect_pdf_type(self, pdf_text: str) -> Dict[str, str]:
        """PDF ìœ í˜• ìë™ ê°ì§€"""
        try:
            result = self.type_detector(text=pdf_text)
            return {
                "pdf_type": getattr(result, "pdf_type", "general_document"),
                "document_category": getattr(result, "document_category", "ì¼ë°˜ë¬¸ì„œ")
            }
        except Exception as e:
            # ê¸°ë³¸ ê°ì§€ ë¡œì§
            if "ê±´ì¶•ê³„íšì„œ" in pdf_text or "ê±´ì¶•ë„ë©´" in pdf_text:
                return {"pdf_type": "architectural_plan", "document_category": "ê±´ì¶•ê³„íšì„œ"}
            elif "í† ì§€ì´ìš©ê³„íš" in pdf_text or "ì§€êµ¬ë‹¨ìœ„ê³„íš" in pdf_text:
                return {"pdf_type": "land_use_plan", "document_category": "í† ì§€ì´ìš©ê³„íš"}
            elif "í™˜ê²½ì˜í–¥í‰ê°€" in pdf_text or "í™˜ê²½" in pdf_text:
                return {"pdf_type": "environmental_assessment", "document_category": "í™˜ê²½í‰ê°€"}
            else:
                return {"pdf_type": "general_document", "document_category": "ì¼ë°˜ë¬¸ì„œ"}
    
    def validate_and_clean_data(self, extracted_data: Dict[str, str]) -> Dict[str, str]:
        """ì¶”ì¶œëœ ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        cleaned_data = {}
        
        for field, value in extracted_data.items():
            # 1. ë¹ˆ ê°’ í™•ì¸
            if not value or value.strip() == "":
                cleaned_data[field] = self.default_values.get(field, "ì •ë³´ ì—†ìŒ")
                continue
            
            # 2. í˜•ì‹ ê²€ì¦
            if field == "site_area":
                cleaned_data[field] = self.validate_area_format(value)
            elif field == "site_address":
                cleaned_data[field] = self.validate_address_format(value)
            else:
                cleaned_data[field] = value.strip()
            
            # 3. ë‚´ìš© í’ˆì§ˆ ê²€ì¦
            if self.is_low_quality_content(cleaned_data[field]):
                cleaned_data[field] = self.improve_content_quality(cleaned_data[field])
        
        return cleaned_data
    
    def validate_area_format(self, value: str) -> str:
        """ëŒ€ì§€ë©´ì  í˜•ì‹ ê²€ì¦"""
        # ìˆ«ì + ë‹¨ìœ„ íŒ¨í„´ í™•ì¸
        area_pattern = r'(\d+(?:,\d+)*)\s*(ã¡|mÂ²|í‰|ì œê³±ë¯¸í„°)'
        match = re.search(area_pattern, value)
        if match:
            return value
        else:
            return f"ëŒ€ì§€ë©´ì : {value} (í˜•ì‹ ê²€ì¦ í•„ìš”)"
    
    def validate_address_format(self, value: str) -> str:
        """ì£¼ì†Œ í˜•ì‹ ê²€ì¦"""
        # í•œêµ­ ì£¼ì†Œ íŒ¨í„´ í™•ì¸
        address_pattern = r'[ê°€-í£]+ì‹œ\s*[ê°€-í£]+êµ¬\s*[ê°€-í£]+ë™'
        if re.search(address_pattern, value):
            return value
        else:
            return f"ì£¼ì†Œ: {value} (í˜•ì‹ ê²€ì¦ í•„ìš”)"
    
    def is_low_quality_content(self, content: str) -> bool:
        """ë‚®ì€ í’ˆì§ˆì˜ ë‚´ìš©ì¸ì§€ í™•ì¸"""
        if len(content) < 10:
            return True
        if content in self.default_values.values():
            return True
        if "ì •ë³´ ì—†ìŒ" in content or "ì—†ìŒ" in content:
            return True
        return False
    
    def improve_content_quality(self, content: str) -> str:
        """ë‚´ìš© í’ˆì§ˆ ê°œì„ """
        if len(content) < 10:
            return f"{content} (ì¶”ê°€ ì •ë³´ í•„ìš”)"
        return content
    
    def assess_extraction_quality(self, extracted_data: Dict[str, str]) -> Dict[str, Any]:
        """ì¶”ì¶œ í’ˆì§ˆ í‰ê°€"""
        total_fields = len(self.required_fields)
        filled_fields = sum(1 for field in self.required_fields if extracted_data.get(field) and extracted_data[field] not in self.default_values.values())
        
        completeness = (filled_fields / total_fields) * 100
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = min(100, completeness + 20)  # ê¸°ë³¸ 20ì  ë³´ë„ˆìŠ¤
        
        return {
            "completeness": round(completeness, 1),
            "filled_fields": filled_fields,
            "total_fields": total_fields,
            "quality_score": round(quality_score, 1),
            "grade": self.assign_grade(quality_score),
            "confidence_level": self.assign_confidence_level(quality_score)
        }
    
    def assign_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë¶€ì—¬"""
        if score >= 90:
            return "A+"
        elif score >= 80:
            return "A"
        elif score >= 70:
            return "B+"
        elif score >= 60:
            return "B"
        elif score >= 50:
            return "C+"
        else:
            return "C"
    
    def assign_confidence_level(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ì‹ ë¢°ë„ ë¶€ì—¬"""
        if score >= 80:
            return "ë†’ìŒ"
        elif score >= 60:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"
    
    def handle_extraction_failure(self, pdf_text: str, error: Exception) -> Dict[str, str]:
        """ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ìƒì„±"""
        st.warning(f"âš ï¸ PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(error)}")
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ì¶”ì¶œ ì‹œë„
        fallback_data = {}
        
        # ëŒ€ì§€ë©´ì  ì¶”ì¶œ ì‹œë„
        area_match = re.search(r'(\d+(?:,\d+)*)\s*(ã¡|mÂ²|í‰)', pdf_text)
        if area_match:
            fallback_data["site_area"] = f"ëŒ€ì§€ë©´ì : {area_match.group(0)}"
        
        # ì£¼ì†Œ ì¶”ì¶œ ì‹œë„
        address_match = re.search(r'[ê°€-í£]+ì‹œ\s*[ê°€-í£]+êµ¬\s*[ê°€-í£]+ë™', pdf_text)
        if address_match:
            fallback_data["site_address"] = f"ì£¼ì†Œ: {address_match.group(0)}"
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        for field in self.required_fields:
            if field not in fallback_data:
                fallback_data[field] = self.default_values[field]
        
        return fallback_data
    
    def comprehensive_analysis(self, pdf_text: str) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ PDF ë¶„ì„ - Rate Limiting ì²˜ë¦¬ í¬í•¨"""
        for attempt in range(MAX_RETRIES):
            try:
                # 1. PDF ìœ í˜• ê°ì§€
                pdf_type_info = self.detect_pdf_type(pdf_text)
                
                # 2. ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
                summary_result = self.summary_predictor(text=pdf_text)
                site_result = self.site_parser(text=pdf_text)
                
                # 3. ë°ì´í„° ì¶”ì¶œ
                extracted_data = {
                    "site_area": getattr(site_result, "site_area", ""),
                    "site_address": getattr(site_result, "site_address", ""),
                    "site_slope": getattr(site_result, "site_slope", ""),
                    "zoning": getattr(site_result, "zoning", ""),
                    "restrictions": getattr(site_result, "restrictions", ""),
                    "traffic": getattr(site_result, "traffic", ""),
                    "precedent_comparison": getattr(site_result, "precedent_comparison", ""),
                    "risk_factors": getattr(site_result, "risk_factors", "")
                }
                
                # 4. ë°ì´í„° ê²€ì¦ ë° ì •ì œ
                cleaned_data = self.validate_and_clean_data(extracted_data)
                
                # 5. í’ˆì§ˆ í‰ê°€
                quality_assessment = self.assess_extraction_quality(cleaned_data)
                
                return {
                    "summary": getattr(summary_result, "summary", "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                    "site_fields": cleaned_data,
                    "pdf_type": pdf_type_info,
                    "quality": quality_assessment,
                    "metadata": {
                        "analysis_timestamp": datetime.now().isoformat(),
                        "text_length": len(pdf_text),
                        "status": "success"
                    }
                }
                
            except Exception as e:
                # Rate Limit ì˜¤ë¥˜ ì²˜ë¦¬
                if RateLimitHandler.handle_rate_limit_error(e, attempt):
                    continue
                
                # ê³¼ë¶€í•˜ ì˜¤ë¥˜ ì²˜ë¦¬
                if RateLimitHandler.handle_overloaded_error(e, attempt):
                    continue
                
                # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
                if attempt == MAX_RETRIES - 1:
                    st.error(f"âŒ PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì•ˆ ìƒì„±
                    fallback_data = self.handle_extraction_failure(pdf_text, e)
                    
                    return {
                        "summary": "PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                        "site_fields": fallback_data,
                        "pdf_type": {"pdf_type": "unknown", "document_category": "ì•Œ ìˆ˜ ì—†ìŒ"},
                        "quality": {
                            "completeness": 0,
                            "quality_score": 0,
                            "grade": "F",
                            "confidence_level": "ë‚®ìŒ"
                        },
                        "metadata": {
                            "analysis_timestamp": datetime.now().isoformat(),
                            "text_length": len(pdf_text),
                            "status": "error",
                            "error_message": str(e)
                        }
                    }
                
                # ì¼ë°˜ ì˜¤ë¥˜ì˜ ê²½ìš° ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                wait_time = 5 + random.uniform(0, 5)
                st.warning(f"âš ï¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {attempt + 1}/{MAX_RETRIES})")
                time.sleep(wait_time)
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {
            "summary": "PDF ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "site_fields": self.default_values,
            "pdf_type": {"pdf_type": "unknown", "document_category": "ì•Œ ìˆ˜ ì—†ìŒ"},
            "quality": {
                "completeness": 0,
                "quality_score": 0,
                "grade": "F",
                "confidence_level": "ë‚®ìŒ"
            },
            "metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "text_length": len(pdf_text),
                "status": "failed_after_retries"
            }
        }

# === ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ===
analyzer = AdvancedPDFAnalyzer()

# === ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ë“¤ ===

def summarize_pdf(pdf_text: str) -> str:
    """PDF í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±) - Rate Limiting ì²˜ë¦¬ í¬í•¨"""
    for attempt in range(MAX_RETRIES):
        try:
            result = analyzer.comprehensive_analysis(pdf_text)
            return result["summary"]
        except Exception as e:
            # Rate Limit ì˜¤ë¥˜ ì²˜ë¦¬
            if RateLimitHandler.handle_rate_limit_error(e, attempt):
                continue
            
            # ê³¼ë¶€í•˜ ì˜¤ë¥˜ ì²˜ë¦¬
            if RateLimitHandler.handle_overloaded_error(e, attempt):
                continue
            
            # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
            if attempt == MAX_RETRIES - 1:
                return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            
            # ì¼ë°˜ ì˜¤ë¥˜ì˜ ê²½ìš° ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
            wait_time = 5 + random.uniform(0, 5)
            st.warning(f"âš ï¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {attempt + 1}/{MAX_RETRIES})")
            time.sleep(wait_time)
    
    return "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

def extract_site_analysis_fields(pdf_text: str) -> dict:
    """PDFì—ì„œ ëŒ€ì§€ ë° ë²•ê·œ ê´€ë ¨ í•„ë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±) - Rate Limiting ì²˜ë¦¬ í¬í•¨"""
    for attempt in range(MAX_RETRIES):
        try:
            result = analyzer.comprehensive_analysis(pdf_text)
            return result["site_fields"]
        except Exception as e:
            # Rate Limit ì˜¤ë¥˜ ì²˜ë¦¬
            if RateLimitHandler.handle_rate_limit_error(e, attempt):
                continue
            
            # ê³¼ë¶€í•˜ ì˜¤ë¥˜ ì²˜ë¦¬
            if RateLimitHandler.handle_overloaded_error(e, attempt):
                continue
            
            # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
            if attempt == MAX_RETRIES - 1:
                return analyzer.default_values
            
            # ì¼ë°˜ ì˜¤ë¥˜ì˜ ê²½ìš° ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
            wait_time = 5 + random.uniform(0, 5)
            st.warning(f"âš ï¸ í•„ë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {attempt + 1}/{MAX_RETRIES})")
            time.sleep(wait_time)
    
    return analyzer.default_values

# === ìƒˆë¡œìš´ ê³ ê¸‰ í•¨ìˆ˜ë“¤ ===

def analyze_pdf_comprehensive(pdf_text: str) -> Dict[str, Any]:
    """ì¢…í•©ì ì¸ PDF ë¶„ì„ (ìƒˆë¡œìš´ ê³ ê¸‰ ê¸°ëŠ¥)"""
    return analyzer.comprehensive_analysis(pdf_text)

def analyze_pdf_in_chunks(pdf_text: str, chunk_size: int = 4000, max_chunks: int = 20) -> Dict[str, Any]:
    """í° PDFë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„ - ê°œì„ ëœ ë²„ì „"""
    if len(pdf_text) <= chunk_size:
        return analyzer.comprehensive_analysis(pdf_text)
    
    # ëŒ€ìš©ëŸ‰ PDF ê²½ê³ 
    if len(pdf_text) > 100000:  # 10ë§Œì ì´ìƒ
        st.warning("âš ï¸ ë§¤ìš° í° PDFì…ë‹ˆë‹¤. ë¶„ì„ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì²­í¬ í¬ê¸° ì¡°ì • (ë„ˆë¬´ ë§ì€ ì²­í¬ ë°©ì§€)
    if len(pdf_text) > chunk_size * max_chunks:
        chunk_size = len(pdf_text) // max_chunks
        st.warning(f"ğŸ“„ PDFê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì²­í¬ í¬ê¸°ë¥¼ {chunk_size:,}ìë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
    
    st.info(f"ğŸ“„ í° PDFë¥¼ {chunk_size:,}ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„í•©ë‹ˆë‹¤...")
    
    # PDFë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë¬¸ì¥ ê²½ê³„ ê³ ë ¤)
    chunks = []
    current_pos = 0
    
    while current_pos < len(pdf_text):
        end_pos = min(current_pos + chunk_size, len(pdf_text))
        
        # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
        if end_pos < len(pdf_text):
            # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ì—ì„œ ìë¥´ê¸°
            for punct in ['.', '!', '?', '\n\n']:
                last_punct = pdf_text.rfind(punct, current_pos, end_pos)
                if last_punct > current_pos + chunk_size * 0.8:  # 80% ì´ìƒ ì±„ì› ì„ ë•Œë§Œ
                    end_pos = last_punct + 1
                    break
        
        chunk = pdf_text[current_pos:end_pos]
        chunks.append(chunk)
        current_pos = end_pos
    
    total_chunks = len(chunks)
    st.info(f"ì´ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ í”„ë¡œê·¸ë ˆìŠ¤ ë°”
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # ê° ì²­í¬ ë¶„ì„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ í•˜ë‚˜ì”© ì²˜ë¦¬)
    chunk_results = []
    successful_chunks = 0
    
    for i, chunk in enumerate(chunks):
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress = (i + 1) / total_chunks
        progress_bar.progress(progress)
        status_text.text(f"ì²­í¬ {i+1}/{total_chunks} ë¶„ì„ ì¤‘... ({successful_chunks}ê°œ ì„±ê³µ)")
        
        try:
            # ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if len(chunk.strip()) < 100:
                st.info(f"ì²­í¬ {i+1} ê±´ë„ˆë›°ê¸° (ë„ˆë¬´ ì§§ìŒ)")
                continue
                
            result = analyzer.comprehensive_analysis(chunk)
            chunk_results.append(result)
            successful_chunks += 1
            
            # ì„±ê³µë¥ ì´ ë‚®ìœ¼ë©´ ê²½ê³ 
            if i > 0 and successful_chunks / (i + 1) < 0.5:
                st.warning(f"âš ï¸ ì²­í¬ ë¶„ì„ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ({successful_chunks}/{i+1})")
                
        except Exception as e:
            st.warning(f"ì²­í¬ {i+1} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            continue
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ ì œê±°
    progress_bar.empty()
    status_text.empty()
    
    # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
    if successful_chunks == total_chunks:
        st.success(f"âœ… ëª¨ë“  ì²­í¬ ë¶„ì„ ì™„ë£Œ! ({successful_chunks}/{total_chunks})")
    elif successful_chunks > 0:
        st.warning(f"âš ï¸ ë¶€ë¶„ ë¶„ì„ ì™„ë£Œ ({successful_chunks}/{total_chunks})")
    else:
        st.error("âŒ ëª¨ë“  ì²­í¬ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    if not chunk_results:
        return {
            "summary": "ëª¨ë“  ì²­í¬ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            "site_fields": analyzer.default_values,
            "pdf_type": {"pdf_type": "unknown", "document_category": "ì•Œ ìˆ˜ ì—†ìŒ"},
            "quality": {
                "completeness": 0,
                "quality_score": 0,
                "grade": "F",
                "confidence_level": "ë‚®ìŒ"
            },
            "metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "text_length": len(pdf_text),
                "status": "failed_all_chunks",
                "chunks_processed": 0,
                "total_chunks": total_chunks
            }
        }
    
    # ì²­í¬ ê²°ê³¼ í†µí•©
    combined_summary = "\n\n".join([r["summary"] for r in chunk_results if r["summary"]])
    
    # ì‚¬ì´íŠ¸ í•„ë“œ í†µí•© (ê°€ì¥ ì™„ì „í•œ ì •ë³´ ìš°ì„ )
    combined_site_fields = {}
    for field in analyzer.required_fields:
        for result in chunk_results:
            if result["site_fields"].get(field) and result["site_fields"][field] != analyzer.default_values[field]:
                combined_site_fields[field] = result["site_fields"][field]
                break
        if field not in combined_site_fields:
            combined_site_fields[field] = analyzer.default_values[field]
    
    # í’ˆì§ˆ í‰ê°€ í†µí•©
    avg_quality_score = sum(r["quality"]["quality_score"] for r in chunk_results) / len(chunk_results)
    avg_completeness = sum(r["quality"]["completeness"] for r in chunk_results) / len(chunk_results)
    
    combined_quality = {
        "completeness": round(avg_completeness, 1),
        "quality_score": round(avg_quality_score, 1),
        "grade": analyzer.assign_grade(avg_quality_score),
        "confidence_level": analyzer.assign_confidence_level(avg_quality_score)
    }
    
    # PDF íƒ€ì… ê²°ì • (ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚œ íƒ€ì… ì„ íƒ)
    pdf_types = {}
    for result in chunk_results:
        pdf_type = result["pdf_type"]["pdf_type"]
        pdf_types[pdf_type] = pdf_types.get(pdf_type, 0) + 1
    
    most_common_type = max(pdf_types.items(), key=lambda x: x[1])[0] if pdf_types else "unknown"
    
    return {
        "summary": combined_summary,
        "site_fields": combined_site_fields,
        "pdf_type": {"pdf_type": most_common_type, "document_category": "ëŒ€ìš©ëŸ‰ ë¬¸ì„œ"},
        "quality": combined_quality,
        "metadata": {
            "analysis_timestamp": datetime.now().isoformat(),
            "text_length": len(pdf_text),
            "status": "success_chunked",
            "chunks_processed": len(chunk_results),
            "total_chunks": total_chunks,
            "success_rate": round(successful_chunks / total_chunks * 100, 1)
        }
    }

def get_pdf_quality_report(pdf_text: str) -> Dict[str, Any]:
    """PDF í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
    result = analyzer.comprehensive_analysis(pdf_text)
    return {
        "quality_assessment": result["quality"],
        "pdf_type": result["pdf_type"],
        "recommendations": generate_improvement_recommendations(result["quality"])
    }



def generate_improvement_recommendations(quality: Dict[str, Any]) -> List[str]:
    """í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
    recommendations = []
    
    if quality["completeness"] < 50:
        recommendations.append("PDFì— ë” ë§ì€ ê±´ì¶• ê´€ë ¨ ì •ë³´ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    if quality["quality_score"] < 60:
        recommendations.append("PDF ë‚´ìš©ì˜ í’ˆì§ˆì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    if quality["confidence_level"] == "ë‚®ìŒ":
        recommendations.append("PDF ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    if not recommendations:
        recommendations.append("PDF ë¶„ì„ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
    
    return recommendations

def generate_word_report(content, user_inputs):
    """Word ë¬¸ì„œ ë³´ê³ ì„œ ìƒì„± - í‘œ ì²˜ë¦¬ ê°œì„ """
    
    if not DOCX_AVAILABLE:
        raise ImportError("python-docx ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install python-docx'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    # Word ë¬¸ì„œ ìƒì„±
    doc = Document()
    
    # ì œëª© ì„¤ì •
    project_name = user_inputs.get('project_name', 'í”„ë¡œì íŠ¸')
    title = doc.add_heading(f"{project_name} ë¶„ì„ ë³´ê³ ì„œ", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ë‚´ìš© íŒŒì‹± ë° ì¶”ê°€
    paragraphs = content.split('\n\n')
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
            
        # í‘œ í˜•ì‹ ì²˜ë¦¬
        if is_table_format(para):
            table_data, table_title = parse_table_from_text(para)
            if table_data and len(table_data) > 0:
                try:
                    # í‘œ ì œëª©ì´ ìˆìœ¼ë©´ ë¨¼ì € ì¶”ê°€
                    if table_title:
                        doc.add_heading(clean_text_for_pdf(table_title), level=3)
                    
                    # í—¤ë” í–‰ í™•ì¸
                    has_header = is_header_row(table_data[0]) if table_data else False
                    
                    # Word í‘œ ìƒì„±
                    table = doc.add_table(rows=len(table_data), cols=len(table_data[0]))
                    table.style = 'Table Grid'
                    
                    # í‘œ ìŠ¤íƒ€ì¼ ê°œì„ 
                    table.allow_autofit = True
                    
                    # ë°ì´í„° ì±„ìš°ê¸°
                    for i, row in enumerate(table_data):
                        for j, cell in enumerate(row):
                            if i < len(table.rows) and j < len(table.rows[i].cells):
                                cell_text = clean_text_for_pdf(cell)
                                table.rows[i].cells[j].text = cell_text
                                
                                # í—¤ë” í–‰ ìŠ¤íƒ€ì¼ë§ (í—¤ë”ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                                if has_header and i == 0:
                                    cell_obj = table.rows[i].cells[j]
                                    for paragraph in cell_obj.paragraphs:
                                        for run in paragraph.runs:
                                            run.bold = True
                
                except Exception as e:
                    print(f"í‘œ ìƒì„± ì˜¤ë¥˜: {e}")
                    # í‘œ ìƒì„± ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    if table_title:
                        doc.add_paragraph(f"[í‘œ ì œëª©: {table_title}]")
                    doc.add_paragraph(f"[í‘œ ë°ì´í„°: {para[:100]}...]")
                
                doc.add_paragraph()  # í‘œ í›„ ë¹ˆ ì¤„
                continue